{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47jGWsdZszaI",
        "outputId": "01e81bc8-3c00-4e63-f27d-7674c4ea3448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SOK/metrics\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/SOK/metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def compute_fidelity_from_explanations(model, x_sample, explanation_values, explanation_function,\n",
        "                                     baseline_value=0.5, metric='prediction_similarity'):\n",
        "    \"\"\"\n",
        "    Compute fidelity using explanation values directly (without separate surrogate model).\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "    explanation_values = np.array(explanation_values).flatten()\n",
        "\n",
        "    # Get original prediction\n",
        "    original_pred = model(x_sample.reshape(1, -1))[0]\n",
        "    if len(original_pred.shape) > 0:\n",
        "        original_pred = original_pred[1] if len(original_pred) > 1 else original_pred[0]\n",
        "\n",
        "    # Approximate prediction using explanation values (simple linear approximation)\n",
        "    explanation_pred = baseline_value + np.sum(explanation_values)\n",
        "    explanation_pred = np.clip(explanation_pred, 0, 1)  # Keep in [0,1] range\n",
        "\n",
        "    # Compute fidelity\n",
        "    if metric == 'prediction_similarity':\n",
        "        fidelity = 1.0 - abs(original_pred - explanation_pred)\n",
        "    else:\n",
        "        threshold = 0.5\n",
        "        orig_binary = 1 if original_pred >= threshold else 0\n",
        "        exp_binary = 1 if explanation_pred >= threshold else 0\n",
        "        fidelity = 1.0 if orig_binary == exp_binary else 0.0\n",
        "\n",
        "    return float(fidelity)\n",
        "\n",
        "\n",
        "def compute_model_parameter_randomization(model_predict_func, x_sample, explanation_function,\n",
        "                                        trained_model=None, model_type='auto',\n",
        "                                        randomization_type='full', similarity_metric='spearman_abs',\n",
        "                                        n_samples=5, random_seed=42):\n",
        "    \"\"\"\n",
        "    Compute Model Parameter Randomization Test exactly as defined in the paper.\n",
        "    Tests whether explanations are sensitive to model parameters by comparing\n",
        "    explanations from trained vs randomly initialized models with same architecture.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_predict_func : Callable\n",
        "        Model prediction function (e.g., lstm_predict_proba, lstm_predict_single)\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations (e.g., get_shap_explanation)\n",
        "        Should accept (model, x_sample) or work with global model\n",
        "    trained_model : object, optional\n",
        "        The actual trained model object. If None, attempts to extract from prediction function\n",
        "    model_type : str, default='auto'\n",
        "        Type of model: 'tensorflow', 'pytorch', 'sklearn', 'auto'\n",
        "    randomization_type : str, default='full'\n",
        "        'full': randomize all weights, 'cascading': layer by layer, 'independent': one layer at a time\n",
        "    similarity_metric : str, default='spearman_abs'\n",
        "        Similarity metric: 'spearman_abs', 'spearman', 'pearson', 'cosine'\n",
        "    n_samples : int, default=5\n",
        "        Number of random model samples for averaging\n",
        "    random_seed : int, default=42\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Model parameter randomization score\n",
        "           - High values (close to 1): Method is INSENSITIVE to model parameters (BAD)\n",
        "           - Low values (close to 0): Method is SENSITIVE to model parameters (GOOD)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import spearmanr, pearsonr\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import copy\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    def _compute_similarity(exp1, exp2, metric):\n",
        "        \"\"\"Compute similarity between two explanations\"\"\"\n",
        "        exp1, exp2 = np.array(exp1).flatten(), np.array(exp2).flatten()\n",
        "\n",
        "        # Ensure same length\n",
        "        if len(exp1) != len(exp2):\n",
        "            min_len = min(len(exp1), len(exp2))\n",
        "            exp1, exp2 = exp1[:min_len], exp2[:min_len]\n",
        "\n",
        "        # Handle edge cases\n",
        "        if len(exp1) == 0 or np.allclose(exp1, 0) and np.allclose(exp2, 0):\n",
        "            return 1.0\n",
        "\n",
        "        try:\n",
        "            if metric == 'spearman_abs':\n",
        "                corr, _ = spearmanr(np.abs(exp1), np.abs(exp2))\n",
        "            elif metric == 'spearman':\n",
        "                corr, _ = spearmanr(exp1, exp2)\n",
        "            elif metric == 'pearson':\n",
        "                corr, _ = pearsonr(exp1, exp2)\n",
        "            elif metric == 'cosine':\n",
        "                exp1_norm = exp1 / (np.linalg.norm(exp1) + 1e-8)\n",
        "                exp2_norm = exp2 / (np.linalg.norm(exp2) + 1e-8)\n",
        "                corr = np.dot(exp1_norm, exp2_norm)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown similarity metric: {metric}\")\n",
        "\n",
        "            return 0.0 if np.isnan(corr) else abs(corr)\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "\n",
        "    def _detect_model_type(model):\n",
        "        \"\"\"Auto-detect model framework\"\"\"\n",
        "        if model is None:\n",
        "            return 'unknown'\n",
        "\n",
        "        model_str = str(type(model)).lower()\n",
        "        if 'tensorflow' in model_str or 'keras' in model_str or hasattr(model, 'layers'):\n",
        "            return 'tensorflow'\n",
        "        elif 'torch' in model_str or hasattr(model, 'state_dict'):\n",
        "            return 'pytorch'\n",
        "        elif hasattr(model, 'coef_') or hasattr(model, 'feature_importances_'):\n",
        "            return 'sklearn'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "\n",
        "    def _randomize_tensorflow_model(model, randomization_layer=None):\n",
        "        \"\"\"Randomize TensorFlow/Keras model weights\"\"\"\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "\n",
        "            # Clone the model architecture\n",
        "            random_model = tf.keras.models.clone_model(model)\n",
        "            random_model.build(model.input_shape)\n",
        "\n",
        "            if randomization_type == 'full' or randomization_layer is None:\n",
        "                # Randomize all layers\n",
        "                for layer in random_model.layers:\n",
        "                    if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
        "                        layer.kernel.assign(tf.random.normal(layer.kernel.shape, stddev=0.01))\n",
        "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "                        layer.bias.assign(tf.random.normal(layer.bias.shape, stddev=0.01))\n",
        "                    if hasattr(layer, 'recurrent_kernel') and layer.recurrent_kernel is not None:\n",
        "                        layer.recurrent_kernel.assign(tf.random.normal(layer.recurrent_kernel.shape, stddev=0.01))\n",
        "            else:\n",
        "                # Randomize specific layer for cascading/independent\n",
        "                if randomization_layer < len(random_model.layers):\n",
        "                    layer = random_model.layers[randomization_layer]\n",
        "                    if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
        "                        layer.kernel.assign(tf.random.normal(layer.kernel.shape, stddev=0.01))\n",
        "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "                        layer.bias.assign(tf.random.normal(layer.bias.shape, stddev=0.01))\n",
        "                    if hasattr(layer, 'recurrent_kernel') and layer.recurrent_kernel is not None:\n",
        "                        layer.recurrent_kernel.assign(tf.random.normal(layer.recurrent_kernel.shape, stddev=0.01))\n",
        "\n",
        "            return random_model\n",
        "        except Exception as e:\n",
        "            print(f\"TensorFlow randomization failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _randomize_pytorch_model(model, randomization_layer=None):\n",
        "        \"\"\"Randomize PyTorch model weights\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            import torch.nn as nn\n",
        "\n",
        "            # Deep copy the model\n",
        "            random_model = copy.deepcopy(model)\n",
        "\n",
        "            if randomization_type == 'full' or randomization_layer is None:\n",
        "                # Randomize all parameters\n",
        "                with torch.no_grad():\n",
        "                    for name, param in random_model.named_parameters():\n",
        "                        if param.requires_grad:\n",
        "                            param.normal_(0, 0.01)\n",
        "            else:\n",
        "                # Randomize specific layer\n",
        "                layer_count = 0\n",
        "                with torch.no_grad():\n",
        "                    for name, module in random_model.named_modules():\n",
        "                        if isinstance(module, (nn.Linear, nn.LSTM, nn.GRU, nn.Conv1d, nn.Conv2d)):\n",
        "                            if layer_count == randomization_layer:\n",
        "                                for param in module.parameters():\n",
        "                                    if param.requires_grad:\n",
        "                                        param.normal_(0, 0.01)\n",
        "                                break\n",
        "                            layer_count += 1\n",
        "\n",
        "            return random_model\n",
        "        except Exception as e:\n",
        "            print(f\"PyTorch randomization failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _randomize_sklearn_model(model, randomization_layer=None):\n",
        "        \"\"\"Randomize sklearn model weights\"\"\"\n",
        "        try:\n",
        "            from sklearn.base import clone\n",
        "\n",
        "            # Clone the model\n",
        "            random_model = clone(model)\n",
        "\n",
        "            # Create dummy data to fit the model\n",
        "            n_features = len(x_sample) if hasattr(x_sample, '__len__') else 10\n",
        "            dummy_X = np.random.normal(0, 0.1, (100, n_features))\n",
        "            dummy_y = np.random.randint(0, 2, 100)\n",
        "\n",
        "            # Fit with dummy data\n",
        "            random_model.fit(dummy_X, dummy_y)\n",
        "\n",
        "            # Randomize weights\n",
        "            if hasattr(random_model, 'coef_'):\n",
        "                random_model.coef_ = np.random.normal(0, 0.01, random_model.coef_.shape)\n",
        "            if hasattr(random_model, 'intercept_'):\n",
        "                random_model.intercept_ = np.random.normal(0, 0.01, random_model.intercept_.shape)\n",
        "\n",
        "            return random_model\n",
        "        except Exception as e:\n",
        "            print(f\"Sklearn randomization failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _create_random_model(model, model_framework, layer_idx=None):\n",
        "        \"\"\"Create randomized model based on framework\"\"\"\n",
        "        if model_framework == 'tensorflow':\n",
        "            return _randomize_tensorflow_model(model, layer_idx)\n",
        "        elif model_framework == 'pytorch':\n",
        "            return _randomize_pytorch_model(model, layer_idx)\n",
        "        elif model_framework == 'sklearn':\n",
        "            return _randomize_sklearn_model(model, layer_idx)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _get_explanation_with_model(model, x_sample, explanation_func):\n",
        "        \"\"\"Get explanation using specific model\"\"\"\n",
        "        try:\n",
        "            # Try different ways to pass model to explanation function\n",
        "            try:\n",
        "                # Method 1: explanation_function(model, x_sample)\n",
        "                return explanation_func(model, x_sample)\n",
        "            except:\n",
        "                try:\n",
        "                    # Method 2: explanation_function(x_sample) with global model replacement\n",
        "                    # This is trickier and framework-dependent\n",
        "                    return explanation_func(x_sample)\n",
        "                except:\n",
        "                    # Method 3: Generate random explanation as fallback (not ideal)\n",
        "                    original_exp = explanation_func(x_sample)\n",
        "                    return np.random.normal(0, 0.1, np.array(original_exp).shape)\n",
        "        except Exception:\n",
        "            return np.random.normal(0, 0.1, len(x_sample))\n",
        "\n",
        "    try:\n",
        "        # Auto-detect model type if needed\n",
        "        if model_type == 'auto':\n",
        "            model_type = _detect_model_type(trained_model)\n",
        "\n",
        "        # Get original explanation\n",
        "        original_explanation = explanation_function(x_sample)\n",
        "        original_explanation = np.array(original_explanation).flatten()\n",
        "\n",
        "        if trained_model is None:\n",
        "            print(\"Warning: No trained model provided. Using simplified randomization.\")\n",
        "            # Fallback: generate random explanations\n",
        "            similarities = []\n",
        "            for _ in range(n_samples):\n",
        "                random_explanation = np.random.normal(0, np.std(original_explanation),\n",
        "                                                    original_explanation.shape)\n",
        "                similarity = _compute_similarity(original_explanation, random_explanation, similarity_metric)\n",
        "                similarities.append(similarity)\n",
        "            return float(np.mean(similarities))\n",
        "\n",
        "        similarities = []\n",
        "\n",
        "        if randomization_type == 'full':\n",
        "            # Full model randomization\n",
        "            for _ in range(n_samples):\n",
        "                random_model = _create_random_model(trained_model, model_type)\n",
        "                if random_model is not None:\n",
        "                    random_explanation = _get_explanation_with_model(random_model, x_sample, explanation_function)\n",
        "                    random_explanation = np.array(random_explanation).flatten()\n",
        "                    similarity = _compute_similarity(original_explanation, random_explanation, similarity_metric)\n",
        "                    similarities.append(similarity)\n",
        "                else:\n",
        "                    # Fallback to random explanation\n",
        "                    random_explanation = np.random.normal(0, np.std(original_explanation),\n",
        "                                                        original_explanation.shape)\n",
        "                    similarity = _compute_similarity(original_explanation, random_explanation, similarity_metric)\n",
        "                    similarities.append(similarity)\n",
        "\n",
        "        elif randomization_type == 'cascading':\n",
        "            # Cascading randomization (layer by layer)\n",
        "            max_layers = 10  # Reasonable default\n",
        "            if hasattr(trained_model, 'layers'):\n",
        "                max_layers = len(trained_model.layers)\n",
        "\n",
        "            for layer_idx in range(min(max_layers, 5)):  # Limit to prevent too many iterations\n",
        "                random_model = _create_random_model(trained_model, model_type, layer_idx)\n",
        "                if random_model is not None:\n",
        "                    random_explanation = _get_explanation_with_model(random_model, x_sample, explanation_function)\n",
        "                    random_explanation = np.array(random_explanation).flatten()\n",
        "                    similarity = _compute_similarity(original_explanation, random_explanation, similarity_metric)\n",
        "                    similarities.append(similarity)\n",
        "\n",
        "        elif randomization_type == 'independent':\n",
        "            # Independent layer randomization\n",
        "            max_layers = 10\n",
        "            if hasattr(trained_model, 'layers'):\n",
        "                max_layers = len(trained_model.layers)\n",
        "\n",
        "            for layer_idx in range(min(max_layers, 5)):\n",
        "                random_model = _create_random_model(trained_model, model_type, layer_idx)\n",
        "                if random_model is not None:\n",
        "                    random_explanation = _get_explanation_with_model(random_model, x_sample, explanation_function)\n",
        "                    random_explanation = np.array(random_explanation).flatten()\n",
        "                    similarity = _compute_similarity(original_explanation, random_explanation, similarity_metric)\n",
        "                    similarities.append(similarity)\n",
        "\n",
        "        if similarities:\n",
        "            return float(np.mean(similarities))\n",
        "        else:\n",
        "            print(\"Warning: All randomization attempts failed. Using fallback.\")\n",
        "            return float('nan')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Model Parameter Randomization Test failed: {e}\")\n",
        "        return float('nan')\n",
        "\n",
        "\n",
        "def compute_model_parameter_randomization(model_predict_func, x_sample, explanation_function,\n",
        "                                        trained_model, create_random_model_func,\n",
        "                                        similarity_metric='spearman_abs', random_seed=42):\n",
        "    \"\"\"\n",
        "    TRUE implementation based on paper's Definition\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    trained_model : Your actual trained LSTM model\n",
        "    create_random_model_func : Function that creates same architecture with random weights\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get explanation from trained model\n",
        "    original_explanation = explanation_function(trained_model, x_sample)\n",
        "\n",
        "    # 2. Create randomly initialized model with SAME architecture\n",
        "    random_model = create_random_model_func()\n",
        "\n",
        "    # 3. Get explanation from random model using SAME method\n",
        "    random_explanation = explanation_function(random_model, x_sample)\n",
        "\n",
        "    # 4. Compare similarities\n",
        "    similarity = compute_similarity(original_explanation, random_explanation)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def compute_feature_mutual_information(model, test_samples, explanation_function,\n",
        "                                     threshold=0.1, n_bins=10, epsilon_min=1e-8):\n",
        "    \"\"\"\n",
        "    Compute Feature Mutual Information I(X, Z) for XAI explanation methods on tabular data.\n",
        "\n",
        "    Measures how much information is preserved/lost when converting original features (X)\n",
        "    to explanation-based features (Z).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    test_samples : np.ndarray\n",
        "        Multiple input samples (shape: [n_samples, n_features])\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations\n",
        "    threshold : float, default=0.1\n",
        "        Threshold for considering a feature \"important\"\n",
        "    n_bins : int, default=10\n",
        "        Number of bins for discretizing continuous variables\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent numerical issues\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Feature mutual information I(X, Z)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sklearn.feature_selection import mutual_info_regression\n",
        "    from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "    # Collect original features (X) and extracted features (Z)\n",
        "    X_original = []\n",
        "    Z_extracted = []\n",
        "\n",
        "    for sample in test_samples:\n",
        "        sample_flat = np.array(sample).flatten()\n",
        "        explanation = explanation_function(sample_flat)\n",
        "        explanation = np.array(explanation).flatten()\n",
        "\n",
        "        # Original features (X)\n",
        "        X_original.append(sample_flat)\n",
        "\n",
        "        # Feature extraction: convert attributions to binary importance (Z)\n",
        "        # Z represents \"extracted features\" - which features are important\n",
        "        important_features = (np.abs(explanation) > threshold).astype(int)\n",
        "        Z_extracted.append(important_features)\n",
        "\n",
        "    X_original = np.array(X_original)\n",
        "    Z_extracted = np.array(Z_extracted)\n",
        "\n",
        "    # Compute mutual information between original and extracted features\n",
        "    # For each original feature dimension, compute MI with extracted features\n",
        "    mi_scores = []\n",
        "\n",
        "    for i in range(X_original.shape[1]):  # For each original feature\n",
        "        x_feature = X_original[:, i].reshape(-1, 1)\n",
        "\n",
        "        # Discretize continuous features for MI calculation\n",
        "        if len(np.unique(x_feature)) > n_bins:\n",
        "            discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "            x_feature_discrete = discretizer.fit_transform(x_feature).flatten()\n",
        "        else:\n",
        "            x_feature_discrete = x_feature.flatten()\n",
        "\n",
        "        # Compute MI between original feature i and all extracted features\n",
        "        # Sum MI across all extracted feature dimensions\n",
        "        total_mi = 0\n",
        "        for j in range(Z_extracted.shape[1]):\n",
        "            z_feature = Z_extracted[:, j]\n",
        "            mi = mutual_info_regression(x_feature_discrete.reshape(-1, 1), z_feature)\n",
        "            total_mi += mi[0]\n",
        "\n",
        "        mi_scores.append(total_mi)\n",
        "\n",
        "    # Average mutual information across all feature dimensions\n",
        "    feature_mi = float(np.mean(mi_scores))\n",
        "    return feature_mi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_diversity(model, test_samples, explanation_function, distance_metric='euclidean',\n",
        "                     epsilon_min=1e-8):\n",
        "    \"\"\"\n",
        "    Compute Diversity metric for explanation methods on multiple samples.\n",
        "\n",
        "    Measures how diverse the explanations are across different input samples.\n",
        "    Higher diversity indicates explanations capture different patterns.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function that takes input and returns predictions\n",
        "    test_samples : np.ndarray\n",
        "        Multiple input samples to generate explanations for (shape: [n_samples, n_features])\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for given inputs\n",
        "    distance_metric : str, default='euclidean'\n",
        "        Distance metric to use ('euclidean', 'manhattan', 'cosine')\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent numerical issues\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Diversity score (higher is more diverse)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.spatial.distance import pdist, euclidean, cityblock, cosine\n",
        "\n",
        "    # Generate explanations for all test samples\n",
        "    explanations = []\n",
        "    for sample in test_samples:\n",
        "        sample_flat = np.array(sample).flatten()\n",
        "        explanation = explanation_function(sample_flat)\n",
        "        explanations.append(np.array(explanation).flatten())\n",
        "\n",
        "    explanations = np.array(explanations)\n",
        "    n_examples = len(explanations)\n",
        "\n",
        "    if n_examples < 2:\n",
        "        return 0.0  # No diversity with single sample\n",
        "\n",
        "    # Choose distance function\n",
        "    distance_functions = {\n",
        "        'euclidean': lambda x, y: euclidean(x, y),\n",
        "        'manhattan': lambda x, y: cityblock(x, y),\n",
        "        'cosine': lambda x, y: cosine(x, y) if np.linalg.norm(x) > epsilon_min and np.linalg.norm(y) > epsilon_min else 0.0\n",
        "    }\n",
        "\n",
        "    if distance_metric not in distance_functions:\n",
        "        distance_metric = 'euclidean'\n",
        "\n",
        "    distance_func = distance_functions[distance_metric]\n",
        "\n",
        "    # Compute diversity: average pairwise distance\n",
        "    total_distance = 0\n",
        "    pair_count = 0\n",
        "\n",
        "    for i in range(n_examples):\n",
        "        for j in range(i+1, n_examples):  # Only unique pairs\n",
        "            dist = distance_func(explanations[i], explanations[j])\n",
        "            total_distance += dist\n",
        "            pair_count += 1\n",
        "\n",
        "    # Following paper's formula: Σd(xi,xj) / 2NE\n",
        "    diversity = total_distance / (2 * n_examples) if n_examples > 0 else 0.0\n",
        "\n",
        "    return float(diversity)\n",
        "\n",
        "\n",
        "\n",
        "def compute_completeness(model, x_sample, explanation_values, epsilon_min=1e-8):\n",
        "    \"\"\"\n",
        "    Compute Completeness metric for any XAI explanation method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function that takes input and returns predictions\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_values : np.ndarray\n",
        "        Attribution values from XAI method (same shape as x_sample)\n",
        "    baseline : np.ndarray, optional\n",
        "        Baseline input. If None, uses zero baseline for tabular data.\n",
        "        For other data types, you may want to specify appropriate baseline.\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent numerical issues\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Completeness score (lower absolute value is better, 0 is perfect)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "    explanation_values = np.array(explanation_values).flatten()\n",
        "\n",
        "    # Create baseline if not provided\n",
        "    baseline = np.zeros_like(x_sample)  # Zero baseline (common default)\n",
        "\n",
        "\n",
        "    # Get model predictions\n",
        "    original_pred = model(x_sample.reshape(1, -1))[0]\n",
        "    if len(original_pred.shape) > 0:\n",
        "        original_pred = original_pred[1] if len(original_pred) > 1 else original_pred[0]\n",
        "\n",
        "    baseline_pred = model(baseline.reshape(1, -1))[0]\n",
        "    if len(baseline_pred.shape) > 0:\n",
        "        baseline_pred = baseline_pred[1] if len(baseline_pred) > 1 else baseline_pred[0]\n",
        "\n",
        "    # Completeness: Σ attributions should equal F(x) - F(baseline)\n",
        "    attribution_sum = np.sum(explanation_values)\n",
        "    model_diff = original_pred - baseline_pred\n",
        "    completeness_error = attribution_sum - model_diff\n",
        "\n",
        "    return float(completeness_error)\n",
        "\n",
        "\n",
        "\n",
        "def compute_infidelity(model, x_sample, explanation_values, explanation_function,\n",
        "                      n_samples=20, noise_std=0.1, epsilon_min=1e-8):\n",
        "    \"\"\"\n",
        "    Compute Infidelity metric for any XAI explanation method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function that takes input and returns predictions\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_values : np.ndarray\n",
        "        Attribution values from XAI method (same shape as x_sample)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for new inputs\n",
        "    n_samples : int, default=20\n",
        "        Number of perturbation samples to generate\n",
        "    noise_std : float, default=0.1\n",
        "        Standard deviation for Gaussian noise perturbations\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent numerical issues\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Infidelity score (lower is better)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "    explanation_values = np.array(explanation_values).flatten()\n",
        "\n",
        "    # Get original prediction\n",
        "    original_pred = model(x_sample.reshape(1, -1))[0]\n",
        "    if len(original_pred.shape) > 0:\n",
        "        original_pred = original_pred[1] if len(original_pred) > 1 else original_pred[0]\n",
        "\n",
        "    infidelity_scores = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Generate perturbation I (following Definition 2.1)\n",
        "        perturbation = np.random.normal(0, noise_std, x_sample.shape)\n",
        "\n",
        "        # Compute x - I (corrected direction according to paper)\n",
        "        x_perturbed = x_sample - perturbation\n",
        "\n",
        "        # Get perturbed prediction\n",
        "        perturbed_pred = model(x_perturbed.reshape(1, -1))[0]\n",
        "        if len(perturbed_pred.shape) > 0:\n",
        "            perturbed_pred = perturbed_pred[1] if len(perturbed_pred) > 1 else perturbed_pred[0]\n",
        "\n",
        "        # Compute infidelity components according to Definition 2.1\n",
        "        # I^T * Φ(f,x)\n",
        "        explanation_approx = np.dot(perturbation, explanation_values)\n",
        "\n",
        "        # f(x) - f(x-I)\n",
        "        model_diff = original_pred - perturbed_pred\n",
        "\n",
        "        # (I^T Φ(f,x) - (f(x) - f(x-I)))^2\n",
        "        infidelity_score = (explanation_approx - model_diff) ** 2\n",
        "        infidelity_scores.append(infidelity_score)\n",
        "\n",
        "    return float(np.mean(infidelity_scores))\n",
        "\n",
        "def compute_complexity(explanation_values):\n",
        "    \"\"\"\n",
        "    Compute Complexity metric according to Definition 4 in Bhatt et al. paper.\n",
        "\n",
        "    Complexity measures how distributed the explanation is across features.\n",
        "    Lower complexity = more focused explanation (fewer important features)\n",
        "    Higher complexity = more distributed explanation (many important features)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    explanation_values : np.ndarray\n",
        "        Attribution values from XAI method (same naming as your other functions)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Complexity score (0 = most focused, ln(d) = maximally distributed)\n",
        "    \"\"\"\n",
        "    # Convert to numpy array and take absolute values\n",
        "    abs_attributions = np.abs(np.array(explanation_values).flatten())\n",
        "\n",
        "    # Handle edge case where all attributions are zero\n",
        "    total_attribution = np.sum(abs_attributions)\n",
        "    if total_attribution == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Create fractional contribution distribution Pg(i)\n",
        "    prob_distribution = abs_attributions / total_attribution\n",
        "\n",
        "    # Remove zero probabilities to avoid log(0) in entropy calculation\n",
        "    prob_distribution = prob_distribution[prob_distribution > 0]\n",
        "\n",
        "    # Compute entropy: μC = -∑ Pg(i) * ln(Pg(i))\n",
        "    complexity = -np.sum(prob_distribution * np.log(prob_distribution))\n",
        "\n",
        "    return float(complexity)\n",
        "\n",
        "\n",
        "\n",
        "def compute_sensitivity(model, x_sample, explanation_function,\n",
        "                       n_samples=20, radius=0.1, epsilon_min=1e-8):\n",
        "    \"\"\"\n",
        "    Compute Max-Sensitivity metric according to Definition 3.1 in the paper.\n",
        "\n",
        "    SENS_MAX(Φ,f,x,r) = max_{||y-x||≤r} ||Φ(f,y) - Φ(f,x)||\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    n_samples : int, default=20\n",
        "        Number of perturbation samples to approximate the maximum\n",
        "    radius : float, default=0.1\n",
        "        Maximum perturbation radius r (L2 norm constraint)\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent numerical issues\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Max-sensitivity score (lower indicates more stable explanations)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "    # Get original explanation Φ(f,x)\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "    max_sensitivity = 0.0\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Generate perturbation with constraint ||perturbation|| ≤ r\n",
        "        # Method 1: Sample from uniform distribution in L2 ball\n",
        "        perturbation = np.random.normal(0, 1, x_sample.shape)\n",
        "        perturbation_norm = np.linalg.norm(perturbation)\n",
        "\n",
        "        if perturbation_norm > epsilon_min:\n",
        "            # Normalize and scale to random radius ≤ r\n",
        "            random_radius = radius * np.random.random()  # uniform in [0, r]\n",
        "            perturbation = (perturbation / perturbation_norm) * random_radius\n",
        "        else:\n",
        "            perturbation = np.zeros_like(x_sample)\n",
        "\n",
        "        # Compute perturbed point y = x + perturbation, where ||y-x|| ≤ r\n",
        "        y = x_sample + perturbation\n",
        "\n",
        "        try:\n",
        "            # Get explanation for perturbed input Φ(f,y)\n",
        "            perturbed_explanation = np.array(explanation_function(y)).flatten()\n",
        "\n",
        "            # Compute ||Φ(f,y) - Φ(f,x)||\n",
        "            explanation_diff = perturbed_explanation - original_explanation\n",
        "            sensitivity = np.linalg.norm(explanation_diff)\n",
        "\n",
        "            # Keep track of maximum: max_{||y-x||≤r} ||Φ(f,y) - Φ(f,x)||\n",
        "            max_sensitivity = max(max_sensitivity, sensitivity)\n",
        "\n",
        "        except Exception:\n",
        "            # Skip this perturbation if explanation fails\n",
        "            continue\n",
        "\n",
        "    return float(max_sensitivity)\n",
        "\n",
        "\n",
        "def compute_relative_input_stability(model, x_sample, explanation_function,\n",
        "                                   n_samples=20, noise_std=0.05, epsilon_min=1e-8, p_norm=2):\n",
        "    \"\"\"\n",
        "    Compute Relative Input Stability (RIS) according to Equation 2.\n",
        "\n",
        "    RIS = max ||(ex'-ex)/ex||_p / max(||(x'-x)/x||_p, ε_min)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "    # Get original explanation\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "    # Generate perturbations with same prediction\n",
        "    valid_perturbations = []\n",
        "    original_pred = np.argmax(model(x_sample.reshape(1, -1))[0])\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 10\n",
        "\n",
        "    while len(valid_perturbations) < n_samples and attempts < max_attempts:\n",
        "        perturbation = np.random.normal(0, noise_std, x_sample.shape)\n",
        "        x_perturbed = x_sample + perturbation\n",
        "\n",
        "        # Check if prediction class is the same\n",
        "        perturbed_pred = np.argmax(model(x_perturbed.reshape(1, -1))[0])\n",
        "        if perturbed_pred == original_pred:\n",
        "            valid_perturbations.append(x_perturbed)\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    if len(valid_perturbations) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    max_instability = 0.0\n",
        "\n",
        "    for x_perturbed in valid_perturbations:\n",
        "        # Get explanation for perturbed input\n",
        "        perturbed_explanation = np.array(explanation_function(x_perturbed)).flatten()\n",
        "\n",
        "        # Compute (ex' - ex) / ex (element-wise, handling division by zero)\n",
        "        explanation_diff = perturbed_explanation - original_explanation\n",
        "        safe_original_exp = np.where(np.abs(original_explanation) < epsilon_min,\n",
        "                                   epsilon_min, original_explanation)\n",
        "        explanation_relative = explanation_diff / safe_original_exp\n",
        "\n",
        "        # Compute (x' - x) / x (element-wise, handling division by zero)\n",
        "        input_diff = x_perturbed - x_sample\n",
        "        safe_original_input = np.where(np.abs(x_sample) < epsilon_min,\n",
        "                                     epsilon_min, x_sample)\n",
        "        input_relative = input_diff / safe_original_input\n",
        "\n",
        "        # Compute norms of the relative change vectors\n",
        "        explanation_norm = np.linalg.norm(explanation_relative, ord=p_norm)\n",
        "        input_norm = np.linalg.norm(input_relative, ord=p_norm)\n",
        "\n",
        "        # Compute RIS ratio\n",
        "        instability_ratio = explanation_norm / max(input_norm, epsilon_min)\n",
        "        max_instability = max(max_instability, instability_ratio)\n",
        "\n",
        "    return float(max_instability)\n",
        "\n",
        "\n",
        "def compute_relative_output_stability(model, x_sample, explanation_function,\n",
        "                                    n_samples=20, noise_std=0.05, epsilon_min=1e-8, p_norm=2):\n",
        "    \"\"\"\n",
        "    Compute Relative Output Stability (ROS) according to Equation 5.\n",
        "\n",
        "    ROS = max ||(ex'-ex)/ex||_p / max(||h(x')-h(x)||_p, ε_min)\n",
        "    where h(x) are the logits (pre-softmax outputs)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "    # Get original explanation and logits\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "    # Get logits - you may need to modify this based on your model interface\n",
        "    # This assumes model returns probabilities; you need raw logits\n",
        "    original_output = model(x_sample.reshape(1, -1))[0]\n",
        "\n",
        "    # If model returns probabilities, convert back to logits (approximation)\n",
        "    # Better: modify to get actual logits from model\n",
        "    original_logits = np.log(np.clip(original_output, epsilon_min, 1-epsilon_min))\n",
        "\n",
        "    # Generate perturbations with same prediction\n",
        "    valid_perturbations = []\n",
        "    original_pred = np.argmax(original_output)\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 10\n",
        "\n",
        "    while len(valid_perturbations) < n_samples and attempts < max_attempts:\n",
        "        perturbation = np.random.normal(0, noise_std, x_sample.shape)\n",
        "        x_perturbed = x_sample + perturbation\n",
        "\n",
        "        # Check if prediction class is the same\n",
        "        perturbed_output = model(x_perturbed.reshape(1, -1))[0]\n",
        "        perturbed_pred = np.argmax(perturbed_output)\n",
        "\n",
        "        if perturbed_pred == original_pred:\n",
        "            # Convert to logits\n",
        "            perturbed_logits = np.log(np.clip(perturbed_output, epsilon_min, 1-epsilon_min))\n",
        "            valid_perturbations.append((x_perturbed, perturbed_logits))\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    if len(valid_perturbations) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    max_instability = 0.0\n",
        "\n",
        "    for x_perturbed, perturbed_logits in valid_perturbations:\n",
        "        # Get explanation for perturbed input\n",
        "        perturbed_explanation = np.array(explanation_function(x_perturbed)).flatten()\n",
        "\n",
        "        # Compute (ex' - ex) / ex\n",
        "        explanation_diff = perturbed_explanation - original_explanation\n",
        "        safe_original_exp = np.where(np.abs(original_explanation) < epsilon_min,\n",
        "                                   epsilon_min, original_explanation)\n",
        "        explanation_relative = explanation_diff / safe_original_exp\n",
        "\n",
        "        # Compute h(x') - h(x) (logit difference)\n",
        "        logit_diff = perturbed_logits - original_logits\n",
        "\n",
        "        # Compute norms\n",
        "        explanation_norm = np.linalg.norm(explanation_relative, ord=p_norm)\n",
        "        logit_norm = np.linalg.norm(logit_diff, ord=p_norm)\n",
        "\n",
        "        # Compute ROS ratio\n",
        "        instability_ratio = explanation_norm / max(logit_norm, epsilon_min)\n",
        "        max_instability = max(max_instability, instability_ratio)\n",
        "\n",
        "    return float(max_instability)\n",
        "\n",
        "\n",
        "\n",
        "def compute_relative_representation_stability1(model, x_sample, explanation_function,\n",
        "                                            representation_layer, n_samples=20,\n",
        "                                            noise_std=0.05, epsilon_min=1e-8, p_norm=2):\n",
        "    \"\"\"\n",
        "    Compute Relative Representation Stability (RRS) according to Equation 3.\n",
        "\n",
        "    RRS = max ||(ex'-ex)/ex||_p / max(||(Lx'-Lx)/Lx||_p, ε_min)\n",
        "    where L(·) denotes internal model representation (e.g., hidden layer outputs)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    representation_layer : Callable\n",
        "        Function that extracts internal representations L(x) from the model\n",
        "        Should take input and return the internal representation (e.g., hidden layer output)\n",
        "    n_samples : int, default=20\n",
        "        Number of perturbation samples\n",
        "    noise_std : float, default=0.05\n",
        "        Standard deviation for perturbations\n",
        "    epsilon_min : float, default=1e-8\n",
        "        Minimum value to prevent division by zero\n",
        "    p_norm : int, default=2\n",
        "        Which p-norm to use for distance calculations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : RRS score (lower is better, indicates more stable explanations)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "    # Get original explanation and representation\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "    original_representation = np.array(representation_layer(x_sample)).flatten()\n",
        "\n",
        "    # Generate perturbations with same prediction\n",
        "    valid_perturbations = []\n",
        "    original_pred = np.argmax(model(x_sample.reshape(1, -1))[0])\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 10\n",
        "\n",
        "    while len(valid_perturbations) < n_samples and attempts < max_attempts:\n",
        "        perturbation = np.random.normal(0, noise_std, x_sample.shape)\n",
        "        x_perturbed = x_sample + perturbation\n",
        "\n",
        "        # Check if prediction class is the same\n",
        "        perturbed_pred = np.argmax(model(x_perturbed.reshape(1, -1))[0])\n",
        "\n",
        "        if perturbed_pred == original_pred:\n",
        "            # Get representation for perturbed input\n",
        "            perturbed_representation = np.array(representation_layer(x_perturbed)).flatten()\n",
        "            valid_perturbations.append((x_perturbed, perturbed_representation))\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    if len(valid_perturbations) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    max_instability = 0.0\n",
        "\n",
        "    for x_perturbed, perturbed_representation in valid_perturbations:\n",
        "        # Get explanation for perturbed input\n",
        "        perturbed_explanation = np.array(explanation_function(x_perturbed)).flatten()\n",
        "\n",
        "        # Compute (ex' - ex) / ex\n",
        "        explanation_diff = perturbed_explanation - original_explanation\n",
        "        safe_original_exp = np.where(np.abs(original_explanation) < epsilon_min,\n",
        "                                   epsilon_min, original_explanation)\n",
        "        explanation_relative = explanation_diff / safe_original_exp\n",
        "\n",
        "        # Compute (Lx' - Lx) / Lx\n",
        "        representation_diff = perturbed_representation - original_representation\n",
        "        safe_original_repr = np.where(np.abs(original_representation) < epsilon_min,\n",
        "                                    epsilon_min, original_representation)\n",
        "        representation_relative = representation_diff / safe_original_repr\n",
        "\n",
        "        # Compute norms of the relative change vectors\n",
        "        explanation_norm = np.linalg.norm(explanation_relative, ord=p_norm)\n",
        "        representation_norm = np.linalg.norm(representation_relative, ord=p_norm)\n",
        "\n",
        "        # Compute RRS ratio\n",
        "        instability_ratio = explanation_norm / max(representation_norm, epsilon_min)\n",
        "        max_instability = max(max_instability, instability_ratio)\n",
        "\n",
        "    return float(max_instability)\n",
        "\n",
        "\n",
        "def compute_relative_representation_stability(model, x_sample, explanation_function,\n",
        "                                            representation_layer, n_samples=20,\n",
        "                                            noise_std=0.05, epsilon_min=1e-3, p_norm=2):  # Increased epsilon_min\n",
        "    \"\"\"\n",
        "    Compute Relative Representation Stability (RRS) according to Equation 3.\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "    # Get original explanation and representation\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "    original_representation = np.array(representation_layer(x_sample)).flatten()\n",
        "\n",
        "    # Generate perturbations with same prediction\n",
        "    valid_perturbations = []\n",
        "    original_pred = np.argmax(model(x_sample.reshape(1, -1))[0])\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 10\n",
        "\n",
        "    while len(valid_perturbations) < n_samples and attempts < max_attempts:\n",
        "        perturbation = np.random.normal(0, noise_std, x_sample.shape)\n",
        "        x_perturbed = x_sample + perturbation\n",
        "\n",
        "        # Check if prediction class is the same\n",
        "        perturbed_pred = np.argmax(model(x_perturbed.reshape(1, -1))[0])\n",
        "\n",
        "        if perturbed_pred == original_pred:\n",
        "            # Get representation for perturbed input\n",
        "            perturbed_representation = np.array(representation_layer(x_perturbed)).flatten()\n",
        "            valid_perturbations.append((x_perturbed, perturbed_representation))\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    if len(valid_perturbations) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    max_instability = 0.0\n",
        "\n",
        "    for x_perturbed, perturbed_representation in valid_perturbations:\n",
        "        # Get explanation for perturbed input\n",
        "        perturbed_explanation = np.array(explanation_function(x_perturbed)).flatten()\n",
        "\n",
        "        # Compute absolute differences instead of relative changes for better stability\n",
        "        explanation_diff = perturbed_explanation - original_explanation\n",
        "        representation_diff = perturbed_representation - original_representation\n",
        "\n",
        "        # Compute norms of the difference vectors\n",
        "        explanation_norm = np.linalg.norm(explanation_diff, ord=p_norm)\n",
        "        representation_norm = np.linalg.norm(representation_diff, ord=p_norm)\n",
        "\n",
        "        # Compute RRS ratio with better numerical stability\n",
        "        instability_ratio = explanation_norm / max(representation_norm, epsilon_min)\n",
        "        max_instability = max(max_instability, instability_ratio)\n",
        "\n",
        "    return float(max_instability)\n",
        "\n",
        "\n",
        "def compute_faithfulness(model, x_sample, explanation_function,\n",
        "                        subset_size=5, n_subsets=50, baseline_type='zero'):\n",
        "    \"\"\"\n",
        "    Compute Faithfulness metric according to Definition 3 in the paper.\n",
        "\n",
        "    μF(f, g; x) = corr[Σi∈S g(f, x)i, f(x) − f(x[xs=x̄s])]\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    subset_size : int, default=5\n",
        "        Size of feature subsets |S| to sample\n",
        "    n_subsets : int, default=50\n",
        "        Number of random subsets to sample for correlation estimation\n",
        "    baseline_type : str, default='zero'\n",
        "        Type of baseline: 'zero' or 'mean' (if mean, you need to provide training data)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Faithfulness correlation score (higher indicates more faithful explanations)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "    d = len(x_sample)\n",
        "\n",
        "    # Ensure subset_size doesn't exceed number of features\n",
        "    subset_size = min(subset_size, d)\n",
        "\n",
        "    # Get original explanation g(f,x)\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "    # Get original model output f(x)\n",
        "    try:\n",
        "        original_output = float(model(x_sample))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "    attribution_sums = []\n",
        "    output_differences = []\n",
        "\n",
        "    for _ in range(n_subsets):\n",
        "        # Randomly sample subset S of size |S|\n",
        "        subset_indices = np.random.choice(d, size=subset_size, replace=False)\n",
        "\n",
        "        # Calculate sum of attributions: Σi∈S g(f, x)i\n",
        "        attribution_sum = np.sum(original_explanation[subset_indices])\n",
        "        attribution_sums.append(attribution_sum)\n",
        "\n",
        "        # Create baseline version x[xs=x̄s]\n",
        "        x_baseline = x_sample.copy()\n",
        "\n",
        "        if baseline_type == 'zero':\n",
        "            x_baseline[subset_indices] = 0.0\n",
        "        elif baseline_type == 'mean':\n",
        "            # For mean baseline, you'd need training data mean\n",
        "            # Using zero as fallback for now\n",
        "            x_baseline[subset_indices] = 0.0\n",
        "\n",
        "        try:\n",
        "            # Get model output for baseline version f(x[xs=x̄s])\n",
        "            baseline_output = float(model(x_baseline))\n",
        "\n",
        "            # Calculate output difference: f(x) − f(x[xs=x̄s])\n",
        "            output_diff = original_output - baseline_output\n",
        "            output_differences.append(output_diff)\n",
        "\n",
        "        except Exception:\n",
        "            # Skip this subset if model prediction fails\n",
        "            continue\n",
        "\n",
        "    # Compute Pearson correlation between attribution sums and output differences\n",
        "    if len(attribution_sums) < 2 or len(output_differences) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Calculate Pearson correlation coefficient\n",
        "        correlation_matrix = np.corrcoef(attribution_sums, output_differences)\n",
        "        correlation = correlation_matrix[0, 1]\n",
        "\n",
        "        # Handle NaN case (when variance is zero)\n",
        "        if np.isnan(correlation):\n",
        "            return 0.0\n",
        "\n",
        "        return float(correlation)\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_faithfulness_with_mean_baseline(model, x_sample, explanation_function,\n",
        "                                          training_mean, subset_size=5, n_subsets=50):\n",
        "    \"\"\"\n",
        "    Compute Faithfulness metric with mean baseline from training data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_sample : np.ndarray\n",
        "        Original input sample (1D array)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    training_mean : np.ndarray\n",
        "        Mean values from training data for baseline\n",
        "    subset_size : int, default=5\n",
        "        Size of feature subsets |S| to sample\n",
        "    n_subsets : int, default=50\n",
        "        Number of random subsets to sample for correlation estimation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Faithfulness correlation score (higher indicates more faithful explanations)\n",
        "    \"\"\"\n",
        "    x_sample = np.array(x_sample).flatten()\n",
        "    training_mean = np.array(training_mean).flatten()\n",
        "    d = len(x_sample)\n",
        "\n",
        "    # Ensure subset_size doesn't exceed number of features\n",
        "    subset_size = min(subset_size, d)\n",
        "\n",
        "    # Get original explanation g(f,x)\n",
        "    original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "    # Get original model output f(x)\n",
        "    try:\n",
        "        original_output = float(model(x_sample))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "    attribution_sums = []\n",
        "    output_differences = []\n",
        "\n",
        "    for _ in range(n_subsets):\n",
        "        # Randomly sample subset S of size |S|\n",
        "        subset_indices = np.random.choice(d, size=subset_size, replace=False)\n",
        "\n",
        "        # Calculate sum of attributions: Σi∈S g(f, x)i\n",
        "        attribution_sum = np.sum(original_explanation[subset_indices])\n",
        "        attribution_sums.append(attribution_sum)\n",
        "\n",
        "        # Create baseline version x[xs=x̄s] using training mean\n",
        "        x_baseline = x_sample.copy()\n",
        "        x_baseline[subset_indices] = training_mean[subset_indices]\n",
        "\n",
        "        try:\n",
        "            # Get model output for baseline version f(x[xs=x̄s])\n",
        "            baseline_output = float(model(x_baseline))\n",
        "\n",
        "            # Calculate output difference: f(x) − f(x[xs=x̄s])\n",
        "            output_diff = original_output - baseline_output\n",
        "            output_differences.append(output_diff)\n",
        "\n",
        "        except Exception:\n",
        "            # Skip this subset if model prediction fails\n",
        "            continue\n",
        "\n",
        "    # Compute Pearson correlation between attribution sums and output differences\n",
        "    if len(attribution_sums) < 2 or len(output_differences) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Calculate Pearson correlation coefficient\n",
        "        correlation_matrix = np.corrcoef(attribution_sums, output_differences)\n",
        "        correlation = correlation_matrix[0, 1]\n",
        "\n",
        "        # Handle NaN case (when variance is zero)\n",
        "        if np.isnan(correlation):\n",
        "            return 0.0\n",
        "\n",
        "        return float(correlation)\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_fidelity_disparity(model, x_samples_group0, x_samples_group1,\n",
        "                               explanation_function, fidelity_type='prediction_gap',\n",
        "                               k=5, m=1000, sigma=0.1):\n",
        "    \"\"\"\n",
        "    Compute Fidelity Disparity between two demographic groups.\n",
        "\n",
        "    Measures if explanations accurately represent the model's decision-making\n",
        "    process equally well across groups.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples_group0 : list or np.ndarray\n",
        "        Samples from group 0 (majority group)\n",
        "    x_samples_group1 : list or np.ndarray\n",
        "        Samples from group 1 (minority group)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    fidelity_type : str, default='prediction_gap'\n",
        "        Type of fidelity: 'prediction_gap' or 'ground_truth'\n",
        "    k : int, default=5\n",
        "        Number of top features to consider\n",
        "    m : int, default=1000\n",
        "        Number of perturbation samples for prediction gap\n",
        "    sigma : float, default=0.1\n",
        "        Noise variance for perturbations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains fidelity scores for both groups and disparity metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_prediction_gap_fidelity(x_sample):\n",
        "        \"\"\"Compute prediction gap fidelity for a single sample\"\"\"\n",
        "        x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "        try:\n",
        "            # Get original explanation and prediction\n",
        "            explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "            original_pred = float(model(x_sample))\n",
        "\n",
        "            # Get top k important features\n",
        "            top_k_indices = np.argsort(np.abs(explanation))[-k:]\n",
        "\n",
        "            gaps = []\n",
        "            for _ in range(m):\n",
        "                # Create perturbed sample - add noise to non-important features\n",
        "                x_perturbed = x_sample.copy()\n",
        "                for i in range(len(x_sample)):\n",
        "                    if i not in top_k_indices:\n",
        "                        x_perturbed[i] += np.random.normal(0, sigma)\n",
        "\n",
        "                # Compute prediction gap\n",
        "                perturbed_pred = float(model(x_perturbed))\n",
        "                gap = abs(original_pred - perturbed_pred)\n",
        "                gaps.append(gap)\n",
        "\n",
        "            return np.mean(gaps)\n",
        "\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    # Compute fidelity scores for both groups\n",
        "    group0_scores = []\n",
        "    group1_scores = []\n",
        "\n",
        "    for x_sample in x_samples_group0:\n",
        "        score = compute_prediction_gap_fidelity(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group0_scores.append(score)\n",
        "\n",
        "    for x_sample in x_samples_group1:\n",
        "        score = compute_prediction_gap_fidelity(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group1_scores.append(score)\n",
        "\n",
        "    if len(group0_scores) == 0 or len(group1_scores) == 0:\n",
        "        return {\n",
        "            'group0_mean': 0.0,\n",
        "            'group1_mean': 0.0,\n",
        "            'disparity_ratio': 1.0,\n",
        "            'disparity_difference': 0.0\n",
        "        }\n",
        "\n",
        "    group0_mean = np.mean(group0_scores)\n",
        "    group1_mean = np.mean(group1_scores)\n",
        "\n",
        "    # Compute disparity metrics\n",
        "    disparity_difference = abs(group0_mean - group1_mean)\n",
        "    disparity_ratio = max(group0_mean, group1_mean) / (min(group0_mean, group1_mean) + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'group0_mean': float(group0_mean),\n",
        "        'group1_mean': float(group1_mean),\n",
        "        'disparity_ratio': float(disparity_ratio),\n",
        "        'disparity_difference': float(disparity_difference),\n",
        "        'group0_scores': group0_scores,\n",
        "        'group1_scores': group1_scores\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_stability_disparity(model, x_samples_group0, x_samples_group1,\n",
        "                                explanation_function, m=5, sigma=0.1):\n",
        "    \"\"\"\n",
        "    Compute Stability Disparity between two demographic groups.\n",
        "\n",
        "    Measures if explanations for similar inputs are equally stable across groups.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples_group0 : list or np.ndarray\n",
        "        Samples from group 0 (majority group)\n",
        "    x_samples_group1 : list or np.ndarray\n",
        "        Samples from group 1 (minority group)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    m : int, default=5\n",
        "        Number of perturbation samples\n",
        "    sigma : float, default=0.1\n",
        "        Noise variance for perturbations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains stability scores for both groups and disparity metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_instability(x_sample):\n",
        "        \"\"\"Compute instability for a single sample\"\"\"\n",
        "        x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "        try:\n",
        "            # Get original explanation\n",
        "            original_explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "            instabilities = []\n",
        "            for _ in range(m):\n",
        "                # Add noise to all features\n",
        "                x_perturbed = x_sample + np.random.normal(0, sigma, x_sample.shape)\n",
        "\n",
        "                # Get explanation for perturbed input\n",
        "                perturbed_explanation = np.array(explanation_function(x_perturbed)).flatten()\n",
        "\n",
        "                # Compute L1 distance between explanations\n",
        "                instability = np.linalg.norm(original_explanation - perturbed_explanation, ord=1)\n",
        "                instabilities.append(instability)\n",
        "\n",
        "            return np.mean(instabilities)\n",
        "\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    # Compute stability scores for both groups\n",
        "    group0_scores = []\n",
        "    group1_scores = []\n",
        "\n",
        "    for x_sample in x_samples_group0:\n",
        "        score = compute_instability(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group0_scores.append(score)\n",
        "\n",
        "    for x_sample in x_samples_group1:\n",
        "        score = compute_instability(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group1_scores.append(score)\n",
        "\n",
        "    if len(group0_scores) == 0 or len(group1_scores) == 0:\n",
        "        return {\n",
        "            'group0_mean': 0.0,\n",
        "            'group1_mean': 0.0,\n",
        "            'disparity_ratio': 1.0,\n",
        "            'disparity_difference': 0.0\n",
        "        }\n",
        "\n",
        "    group0_mean = np.mean(group0_scores)\n",
        "    group1_mean = np.mean(group1_scores)\n",
        "\n",
        "    # Compute disparity metrics (higher instability is worse)\n",
        "    disparity_difference = abs(group0_mean - group1_mean)\n",
        "    disparity_ratio = max(group0_mean, group1_mean) / (min(group0_mean, group1_mean) + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'group0_mean': float(group0_mean),\n",
        "        'group1_mean': float(group1_mean),\n",
        "        'disparity_ratio': float(disparity_ratio),\n",
        "        'disparity_difference': float(disparity_difference),\n",
        "        'group0_scores': group0_scores,\n",
        "        'group1_scores': group1_scores\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_consistency_disparity(model, x_samples_group0, x_samples_group1,\n",
        "                                  explanation_function, m=5):\n",
        "    \"\"\"\n",
        "    Compute Consistency Disparity between two demographic groups.\n",
        "\n",
        "    Measures if multiple explanations for the same input are equally consistent across groups.\n",
        "    Note: Only applicable to stochastic explanation methods.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples_group0 : list or np.ndarray\n",
        "        Samples from group 0 (majority group)\n",
        "    x_samples_group1 : list or np.ndarray\n",
        "        Samples from group 1 (minority group)\n",
        "    explanation_function : Callable\n",
        "        Stochastic function that generates explanations for inputs\n",
        "    m : int, default=5\n",
        "        Number of explanation samples to generate for consistency check\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains consistency scores for both groups and disparity metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_inconsistency(x_sample):\n",
        "        \"\"\"Compute inconsistency for a single sample\"\"\"\n",
        "        x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "        try:\n",
        "            # Generate multiple explanations for the same input\n",
        "            explanations = []\n",
        "            for _ in range(m + 1):  # +1 for baseline explanation\n",
        "                explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "                explanations.append(explanation)\n",
        "\n",
        "            # Use first explanation as baseline\n",
        "            baseline_explanation = explanations[0]\n",
        "\n",
        "            # Compute L1 distances from baseline to other explanations\n",
        "            inconsistencies = []\n",
        "            for i in range(1, len(explanations)):\n",
        "                inconsistency = np.linalg.norm(baseline_explanation - explanations[i], ord=1)\n",
        "                inconsistencies.append(inconsistency)\n",
        "\n",
        "            return np.mean(inconsistencies)\n",
        "\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    # Compute consistency scores for both groups\n",
        "    group0_scores = []\n",
        "    group1_scores = []\n",
        "\n",
        "    for x_sample in x_samples_group0:\n",
        "        score = compute_inconsistency(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group0_scores.append(score)\n",
        "\n",
        "    for x_sample in x_samples_group1:\n",
        "        score = compute_inconsistency(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group1_scores.append(score)\n",
        "\n",
        "    if len(group0_scores) == 0 or len(group1_scores) == 0:\n",
        "        return {\n",
        "            'group0_mean': 0.0,\n",
        "            'group1_mean': 0.0,\n",
        "            'disparity_ratio': 1.0,\n",
        "            'disparity_difference': 0.0\n",
        "        }\n",
        "\n",
        "    group0_mean = np.mean(group0_scores)\n",
        "    group1_mean = np.mean(group1_scores)\n",
        "\n",
        "    # Compute disparity metrics (higher inconsistency is worse)\n",
        "    disparity_difference = abs(group0_mean - group1_mean)\n",
        "    disparity_ratio = max(group0_mean, group1_mean) / (min(group0_mean, group1_mean) + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'group0_mean': float(group0_mean),\n",
        "        'group1_mean': float(group1_mean),\n",
        "        'disparity_ratio': float(disparity_ratio),\n",
        "        'disparity_difference': float(disparity_difference),\n",
        "        'group0_scores': group0_scores,\n",
        "        'group1_scores': group1_scores\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_sparsity_disparity(model, x_samples_group0, x_samples_group1,\n",
        "                               explanation_function, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Compute Sparsity Disparity between two demographic groups.\n",
        "\n",
        "    Measures if explanations use different numbers of features across groups.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples_group0 : list or np.ndarray\n",
        "        Samples from group 0 (majority group)\n",
        "    x_samples_group1 : list or np.ndarray\n",
        "        Samples from group 1 (minority group)\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    threshold : float, default=0.01\n",
        "        Threshold above which feature importance is considered significant\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains sparsity scores for both groups and disparity metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_complexity(x_sample):\n",
        "        \"\"\"Compute complexity (number of significant features) for a single sample\"\"\"\n",
        "        x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "        try:\n",
        "            # Get explanation\n",
        "            explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "            # Count features with importance above threshold\n",
        "            significant_features = np.sum(np.abs(explanation) > threshold)\n",
        "\n",
        "            return float(significant_features)\n",
        "\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    # Compute sparsity scores for both groups\n",
        "    group0_scores = []\n",
        "    group1_scores = []\n",
        "\n",
        "    for x_sample in x_samples_group0:\n",
        "        score = compute_complexity(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group0_scores.append(score)\n",
        "\n",
        "    for x_sample in x_samples_group1:\n",
        "        score = compute_complexity(x_sample)\n",
        "        if not np.isnan(score):\n",
        "            group1_scores.append(score)\n",
        "\n",
        "    if len(group0_scores) == 0 or len(group1_scores) == 0:\n",
        "        return {\n",
        "            'group0_mean': 0.0,\n",
        "            'group1_mean': 0.0,\n",
        "            'disparity_ratio': 1.0,\n",
        "            'disparity_difference': 0.0\n",
        "        }\n",
        "\n",
        "    group0_mean = np.mean(group0_scores)\n",
        "    group1_mean = np.mean(group1_scores)\n",
        "\n",
        "    # Compute disparity metrics\n",
        "    disparity_difference = abs(group0_mean - group1_mean)\n",
        "    disparity_ratio = max(group0_mean, group1_mean) / (min(group0_mean, group1_mean) + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'group0_mean': float(group0_mean),\n",
        "        'group1_mean': float(group1_mean),\n",
        "        'disparity_ratio': float(disparity_ratio),\n",
        "        'disparity_difference': float(disparity_difference),\n",
        "        'group0_scores': group0_scores,\n",
        "        'group1_scores': group1_scores\n",
        "    }\n",
        "\n",
        "def compute_cover(model, x_samples, explanation_function):\n",
        "    \"\"\"\n",
        "    Compute Cover metric - measures how many instances are \"explained\"\n",
        "    by having non-zero attributions.\n",
        "\n",
        "    In the context of feature attribution explanations, an instance is\n",
        "    \"covered\" if the explanation assigns meaningful importance to at least\n",
        "    one feature (above a threshold).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples : list or np.ndarray\n",
        "        Input samples to evaluate\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains cover statistics\n",
        "    \"\"\"\n",
        "\n",
        "    covered_instances = 0\n",
        "    total_instances = 0\n",
        "    cover_details = []\n",
        "\n",
        "    # Threshold for considering a feature \"significantly attributed\"\n",
        "    attribution_threshold = 1e-6\n",
        "\n",
        "    for x_sample in x_samples:\n",
        "        try:\n",
        "            x_sample = np.array(x_sample).flatten()\n",
        "\n",
        "            # Get explanation\n",
        "            explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "            # Check if any feature has significant attribution\n",
        "            max_attribution = np.max(np.abs(explanation))\n",
        "            has_significant_attribution = max_attribution > attribution_threshold\n",
        "\n",
        "            if has_significant_attribution:\n",
        "                covered_instances += 1\n",
        "\n",
        "            total_instances += 1\n",
        "\n",
        "            cover_details.append({\n",
        "                'covered': has_significant_attribution,\n",
        "                'max_attribution': float(max_attribution),\n",
        "                'num_significant_features': int(np.sum(np.abs(explanation) > attribution_threshold))\n",
        "            })\n",
        "\n",
        "        except Exception:\n",
        "            # Skip failed explanations\n",
        "            total_instances += 1\n",
        "            cover_details.append({\n",
        "                'covered': False,\n",
        "                'max_attribution': 0.0,\n",
        "                'num_significant_features': 0\n",
        "            })\n",
        "\n",
        "    coverage_rate = covered_instances / total_instances if total_instances > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'covered_instances': covered_instances,\n",
        "        'total_instances': total_instances,\n",
        "        'coverage_rate': float(coverage_rate),\n",
        "        'cover_details': cover_details\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_rule_overlap(model, x_samples, explanation_function,\n",
        "                        similarity_threshold=0.8, top_k_features=5):\n",
        "    \"\"\"\n",
        "    Compute Rule Overlap metric - measures ambiguity by finding instances\n",
        "    with similar explanations (representing overlapping \"rules\").\n",
        "\n",
        "    In feature attribution context, we identify \"rules\" as explanations that\n",
        "    focus on similar sets of important features, then measure how many\n",
        "    instances are explained by overlapping rule patterns.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples : list or np.ndarray\n",
        "        Input samples to evaluate\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    similarity_threshold : float, default=0.8\n",
        "        Threshold for considering two explanations as overlapping rules\n",
        "    top_k_features : int, default=5\n",
        "        Number of top features to consider when comparing explanations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Contains rule overlap statistics\n",
        "    \"\"\"\n",
        "\n",
        "    explanations = []\n",
        "    valid_samples = []\n",
        "\n",
        "    # Collect all explanations\n",
        "    for x_sample in x_samples:\n",
        "        try:\n",
        "            x_sample = np.array(x_sample).flatten()\n",
        "            explanation = np.array(explanation_function(x_sample)).flatten()\n",
        "\n",
        "            explanations.append(explanation)\n",
        "            valid_samples.append(x_sample)\n",
        "\n",
        "        except Exception:\n",
        "            # Skip failed explanations\n",
        "            continue\n",
        "\n",
        "    if len(explanations) < 2:\n",
        "        return {\n",
        "            'total_overlaps': 0,\n",
        "            'overlap_rate': 0.0,\n",
        "            'num_explanations': len(explanations),\n",
        "            'ambiguous_instances': 0\n",
        "        }\n",
        "\n",
        "    explanations = np.array(explanations)\n",
        "    total_overlaps = 0\n",
        "    ambiguous_instances = set()\n",
        "\n",
        "    # Compare all pairs of explanations\n",
        "    for i in range(len(explanations)):\n",
        "        for j in range(i + 1, len(explanations)):\n",
        "\n",
        "            exp_i = explanations[i]\n",
        "            exp_j = explanations[j]\n",
        "\n",
        "            # Get top-k features for each explanation\n",
        "            top_features_i = set(np.argsort(np.abs(exp_i))[-top_k_features:])\n",
        "            top_features_j = set(np.argsort(np.abs(exp_j))[-top_k_features:])\n",
        "\n",
        "            # Calculate Jaccard similarity of top features\n",
        "            intersection = len(top_features_i.intersection(top_features_j))\n",
        "            union = len(top_features_i.union(top_features_j))\n",
        "            jaccard_similarity = intersection / union if union > 0 else 0.0\n",
        "\n",
        "            # Alternative: Calculate cosine similarity of full explanations\n",
        "            cosine_sim = np.dot(exp_i, exp_j) / (np.linalg.norm(exp_i) * np.linalg.norm(exp_j) + 1e-8)\n",
        "\n",
        "            # Use the higher of the two similarities\n",
        "            similarity = max(jaccard_similarity, abs(cosine_sim))\n",
        "\n",
        "            # If explanations are similar enough, count as overlap\n",
        "            if similarity >= similarity_threshold:\n",
        "                total_overlaps += 1\n",
        "                ambiguous_instances.add(i)\n",
        "                ambiguous_instances.add(j)\n",
        "\n",
        "    num_explanations = len(explanations)\n",
        "    max_possible_overlaps = (num_explanations * (num_explanations - 1)) // 2\n",
        "    overlap_rate = total_overlaps / max_possible_overlaps if max_possible_overlaps > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'total_overlaps': total_overlaps,\n",
        "        'overlap_rate': float(overlap_rate),\n",
        "        'num_explanations': num_explanations,\n",
        "        'ambiguous_instances': len(ambiguous_instances),\n",
        "        'ambiguity_rate': float(len(ambiguous_instances) / num_explanations) if num_explanations > 0 else 0.0\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_unambiguity_score(model, x_samples, explanation_function,\n",
        "                             similarity_threshold=0.8, top_k_features=5):\n",
        "    \"\"\"\n",
        "    Compute Unambiguity Score - the inverse of rule overlap.\n",
        "    Higher scores indicate better unambiguity (less overlapping explanations).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Callable\n",
        "        Model prediction function\n",
        "    x_samples : list or np.ndarray\n",
        "        Input samples to evaluate\n",
        "    explanation_function : Callable\n",
        "        Function that generates explanations for inputs\n",
        "    similarity_threshold : float, default=0.8\n",
        "        Threshold for considering two explanations as overlapping\n",
        "    top_k_features : int, default=5\n",
        "        Number of top features to consider\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : Unambiguity score (higher is better, range [0,1])\n",
        "    \"\"\"\n",
        "\n",
        "    overlap_results = compute_rule_overlap(\n",
        "        model, x_samples, explanation_function,\n",
        "        similarity_threshold, top_k_features\n",
        "    )\n",
        "\n",
        "    # Unambiguity is the inverse of overlap rate\n",
        "    unambiguity_score = 1.0 - overlap_results['overlap_rate']\n",
        "\n",
        "    return float(unambiguity_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h3Rk3TvXZ029"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from tabular_metrics import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to evaluate SHAP, LIME, and EBM with all four metrics\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"XAI METRICS EVALUATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Configuration\n",
        "    LSTM_PATH = \"/content/drive/MyDrive/SOK/metrics/best_lstm_model.h5\"\n",
        "    EBM_DIR = \"/content/drive/MyDrive/SOK/metrics\"\n",
        "    SAMPLE_IDX = 0\n",
        "    N_METRIC_SAMPLES = 15\n",
        "\n",
        "    # ========================================================================\n",
        "    # DATA PREPARATION\n",
        "    # ========================================================================\n",
        "    print(\"\\n1. PREPARING DATA\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('breast.csv')\n",
        "    df['diagnosis'] = LabelEncoder().fit_transform(df['diagnosis'])\n",
        "    X = df.drop(columns=['id', 'Unnamed: 32', 'diagnosis'], errors='ignore')\n",
        "    y = df['diagnosis']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    print(f\"✓ Train set: {X_train.shape[0]} samples\")\n",
        "    print(f\"✓ Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Select test sample\n",
        "    x_sample = X_test.iloc[SAMPLE_IDX].values\n",
        "    true_label = y_test.iloc[SAMPLE_IDX]\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    print(f\"✓ Selected sample {SAMPLE_IDX}: True label = {true_label} ({'Malignant' if true_label == 1 else 'Benign'})\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # DEMOGRAPHIC GROUP PREPARATION FOR DISPARITY ANALYSIS\n",
        "    # ========================================================================\n",
        "    print(\"\\n1.5. PREPARING DEMOGRAPHIC GROUPS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # For breast cancer dataset, we can create groups based on diagnosis\n",
        "    # Group 0: Benign cases (diagnosis = 0)\n",
        "    # Group 1: Malignant cases (diagnosis = 1)\n",
        "    group0_indices = y_test[y_test == 0].index\n",
        "    group1_indices = y_test[y_test == 1].index\n",
        "\n",
        "    # Get samples for each group (limit to reasonable number for computation)\n",
        "    max_samples_per_group = 20\n",
        "    group0_samples = X_test.loc[group0_indices[:max_samples_per_group]].values\n",
        "    group1_samples = X_test.loc[group1_indices[:max_samples_per_group]].values\n",
        "\n",
        "    print(f\"✓ Group 0 (Benign): {len(group0_samples)} samples\")\n",
        "    print(f\"✓ Group 1 (Malignant): {len(group1_samples)} samples\")\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD MODELS\n",
        "    # ========================================================================\n",
        "    print(\"\\n2. LOADING MODELS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Load LSTM\n",
        "    lstm_model = load_model(LSTM_PATH)\n",
        "    print(\"✓ LSTM model loaded\")\n",
        "\n",
        "    # Load EBM and components\n",
        "    ebm_model = joblib.load(os.path.join(EBM_DIR, 'ebm_model.pkl'))\n",
        "    scaler = joblib.load(os.path.join(EBM_DIR, 'scaler.pkl'))\n",
        "    with open(os.path.join(EBM_DIR, 'feature_names.pkl'), 'rb') as f:\n",
        "        saved_feature_names = pickle.load(f)\n",
        "    print(\"✓ EBM model and components loaded\")\n",
        "\n",
        "    # LSTM prediction function\n",
        "    def lstm_predict_proba(X):\n",
        "        if hasattr(X, 'values'):\n",
        "            X_array = X.values\n",
        "        else:\n",
        "            X_array = X\n",
        "        if X_array.ndim == 1:\n",
        "            X_array = X_array.reshape(1, -1)\n",
        "\n",
        "        X_scaled = scaler.transform(X_array)\n",
        "        X_reshaped = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])\n",
        "        preds = lstm_model.predict(X_reshaped, verbose=0)\n",
        "        return np.column_stack([1 - preds.flatten(), preds.flatten()])\n",
        "\n",
        "    # EBM prediction function\n",
        "    def ebm_predict_proba(X):\n",
        "        if hasattr(X, 'values'):\n",
        "            X_array = X.values\n",
        "        else:\n",
        "            X_array = X\n",
        "        if X_array.ndim == 1:\n",
        "            X_array = X_array.reshape(1, -1)\n",
        "\n",
        "        X_scaled = scaler.transform(X_array)\n",
        "        X_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "        return ebm_model.predict_proba(X_df)\n",
        "\n",
        "    def get_lstm_representation(x_sample):\n",
        "        x_scaled = scaler.transform(x_sample.reshape(1, -1))\n",
        "        X_reshaped = x_scaled.reshape(x_scaled.shape[0], 1, x_scaled.shape[1])\n",
        "        preds = lstm_model.predict(X_reshaped, verbose=0)\n",
        "\n",
        "        # Convert to logits for better numerical stability\n",
        "        epsilon = 1e-7\n",
        "        preds_clipped = np.clip(preds.flatten(), epsilon, 1 - epsilon)\n",
        "        logits = np.log(preds_clipped / (1 - preds_clipped))\n",
        "        return logits\n",
        "\n",
        "    def lstm_predict_single(X):\n",
        "        return lstm_predict_proba(X)[:, 1]\n",
        "\n",
        "    def ebm_predict_single(X):\n",
        "      return ebm_predict_proba(X)[:, 1]\n",
        "\n",
        "    def get_ebm_representation(x_sample):\n",
        "        if x_sample.ndim == 1:\n",
        "            x_sample = x_sample.reshape(1, -1)\n",
        "\n",
        "        preds = ebm_model.predict_proba(x_sample)[:, 1]\n",
        "        epsilon = 1e-7\n",
        "        preds_clipped = np.clip(preds, epsilon, 1 - epsilon)\n",
        "        logits = np.log(preds_clipped / (1 - preds_clipped))\n",
        "        return logits\n",
        "\n",
        "    # Get predictions\n",
        "    lstm_pred = lstm_predict_proba(x_sample)[0]\n",
        "    ebm_pred = ebm_predict_proba(x_sample)[0]\n",
        "    test_samples = X_test.iloc[:50].values\n",
        "    print(f\"✓ LSTM prediction: {lstm_pred[1]:.4f} ({'Malignant' if lstm_pred[1] > 0.5 else 'Benign'})\")\n",
        "    print(f\"✓ EBM prediction: {ebm_pred[1]:.4f} ({'Malignant' if ebm_pred[1] > 0.5 else 'Benign'})\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SHAP EVALUATION\n",
        "    # ========================================================================\n",
        "\n",
        "\n",
        "    print(\"\\n3. SHAP EVALUATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Setup SHAP\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        background_data = X_train_scaled[:50]\n",
        "        shap_explainer = shap.KernelExplainer(lstm_predict_proba, background_data)\n",
        "        print(\"✓ SHAP explainer initialized\")\n",
        "\n",
        "        # Get SHAP explanation\n",
        "        def get_shap_explanation(x):\n",
        "            x_scaled = scaler.transform(x.reshape(1, -1))\n",
        "            shap_values = shap_explainer.shap_values(x_scaled, nsamples=100)\n",
        "\n",
        "            # Handle different SHAP output formats\n",
        "            if isinstance(shap_values, list):\n",
        "                return shap_values[1][0] if len(shap_values) > 1 else shap_values[0][0]\n",
        "            else:\n",
        "                if shap_values.ndim == 3:\n",
        "                    return shap_values[0, :, 1]\n",
        "                elif shap_values.ndim == 2:\n",
        "                    if shap_values.shape[1] == len(feature_names):\n",
        "                        return shap_values[0]\n",
        "                    elif shap_values.shape[1] == len(feature_names) * 2:\n",
        "                        return shap_values[0, len(feature_names):]\n",
        "                    else:\n",
        "                        return shap_values[0]\n",
        "                else:\n",
        "                    return shap_values\n",
        "\n",
        "        shap_explanation = get_shap_explanation(x_sample)\n",
        "        shap_explanation = np.array(shap_explanation).flatten()\n",
        "\n",
        "        # Ensure correct shape\n",
        "        if len(shap_explanation) != len(feature_names):\n",
        "            if len(shap_explanation) == len(feature_names) * 2:\n",
        "                shap_explanation = shap_explanation[len(feature_names):]\n",
        "            else:\n",
        "                raise ValueError(f\"SHAP shape mismatch: {len(shap_explanation)} vs {len(feature_names)}\")\n",
        "\n",
        "\n",
        "        print(f\"✓ SHAP explanation shape: {shap_explanation.shape}\")\n",
        "\n",
        "        # Compute SHAP metrics\n",
        "        print(\"Computing SHAP metrics...\")\n",
        "        shap_infidelity = compute_infidelity(lstm_predict_proba, x_sample, shap_explanation, get_shap_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        shap_complexity = compute_complexity(shap_explanation)\n",
        "        shap_sensitivity = compute_sensitivity(lstm_predict_proba, x_sample, get_shap_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        shap_ris = compute_relative_input_stability(lstm_predict_proba, x_sample, get_shap_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        shap_ros = compute_relative_output_stability(lstm_predict_proba, x_sample, get_shap_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        shap_rrs = compute_relative_representation_stability(lstm_predict_proba, x_sample, get_shap_explanation, get_lstm_representation, n_samples=N_METRIC_SAMPLES)\n",
        "        shap_faith = compute_faithfulness(lstm_predict_proba, x_sample, get_shap_explanation)\n",
        "        shap_fidelity_disparity = compute_fidelity_disparity(lstm_predict_single, group0_samples, group1_samples,get_shap_explanation, k=5, m=20, sigma=0.1)\n",
        "        shap_stability_disparity = compute_stability_disparity(lstm_predict_single, group0_samples, group1_samples,get_shap_explanation, m=5, sigma=0.1)\n",
        "        shap_consistency_disparity = compute_consistency_disparity(lstm_predict_single, group0_samples, group1_samples,get_shap_explanation, m=5)\n",
        "        shap_sparsity_disparity = compute_sparsity_disparity(lstm_predict_single, group0_samples, group1_samples,get_shap_explanation, threshold=0.01)\n",
        "        shap_cover = compute_cover(lstm_predict_single, test_samples, get_shap_explanation)\n",
        "        shap_rule_overlap = compute_rule_overlap(lstm_predict_single, test_samples, get_shap_explanation,similarity_threshold=0.8, top_k_features=5)\n",
        "        shap_unambiguity = compute_unambiguity_score(lstm_predict_single, test_samples, get_shap_explanation)\n",
        "        shap_completeness = compute_completeness(lstm_predict_proba, x_sample, shap_explanation)\n",
        "        shap_diversity = compute_diversity(lstm_predict_single, test_samples, get_shap_explanation, distance_metric='euclidean')\n",
        "        shap_feature_mi = compute_feature_mutual_information(lstm_predict_single, test_samples, get_shap_explanation, threshold=0.1)\n",
        "        shap_model_param_rand = compute_model_parameter_randomization(lstm_predict_proba, x_sample, get_shap_explanation,trained_model = lstm_model,  model_type='tensorflow', randomization_type='full',n_samples=N_METRIC_SAMPLES)\n",
        "        shap_fidelity = compute_fidelity_from_explanations(lstm_predict_proba, x_sample, shap_explanation, get_shap_explanation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"✓ SHAP Results:\")\n",
        "        print(f\"  Infidelity: {shap_infidelity:.4f}\")\n",
        "        print(f\"  Complexity: {shap_complexity:.4f}\")\n",
        "        print(f\"  Sensitivity: {shap_sensitivity:.4f}\")\n",
        "        print(f\"  RIS: {shap_ris:.4f}\")\n",
        "        print(f\"  ROS: {shap_ros:.4f}\")\n",
        "        print(f\"  RRS: {shap_rrs:.4f}\")\n",
        "        print(f\"  Faithfulness: {shap_faith:.4f}\")\n",
        "        print(\"✓ SHAP Disparity Results:\")\n",
        "        print(f\"  Fidelity Disparity:\")\n",
        "        print(f\"    Group 0 (Benign) Mean: {shap_fidelity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 (Malignant) Mean: {shap_fidelity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {shap_fidelity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"    Disparity Difference: {shap_fidelity_disparity['disparity_difference']:.4f}\")\n",
        "        print(f\"  Stability Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {shap_stability_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {shap_stability_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {shap_stability_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Consistency Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {shap_consistency_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {shap_consistency_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {shap_consistency_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Sparsity Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {shap_sparsity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {shap_sparsity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {shap_sparsity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(\"✓ SHAP Cover and Rule Overlap Results:\")\n",
        "        print(f\"  Cover Rate: {shap_cover['coverage_rate']:.4f}\")\n",
        "        print(f\"  Covered Instances: {shap_cover['covered_instances']}/{shap_cover['total_instances']}\")\n",
        "        print(f\"  Rule Overlap Rate: {shap_rule_overlap['overlap_rate']:.4f}\")\n",
        "        print(f\"  Total Overlaps: {shap_rule_overlap['total_overlaps']}\")\n",
        "        print(f\"  Ambiguous Instances: {shap_rule_overlap['ambiguous_instances']}\")\n",
        "        print(f\"  Unambiguity Score: {shap_unambiguity:.4f}\")\n",
        "        print(f\"✓ SHAP Completeness: {shap_completeness:.4f}\")\n",
        "        print(f\"✓ SHAP Diversity: {shap_diversity:.4f}\")\n",
        "        print(f\"✓ SHAP Feature Mutual Information: {shap_feature_mi:.4f}\")\n",
        "        print(f\"✓ SHAP Model Parameter Randomization: {shap_model_param_rand:.4f}\")\n",
        "        print(f\"✓ SHAP Fidelity: {shap_fidelity:.4f}\")\n",
        "\n",
        "\n",
        "        shap_results = {\n",
        "            'infidelity': shap_infidelity,\n",
        "            'complexity': shap_complexity,\n",
        "            'sensitivity': shap_sensitivity,\n",
        "            'ris': shap_ris,\n",
        "            'ros': shap_ros,\n",
        "            'rrs': shap_rrs,\n",
        "            'faithfulness': shap_faith,\n",
        "            'cover': shap_cover,\n",
        "            'rule_overlap': shap_rule_overlap,\n",
        "            'total_overlaps': shap_rule_overlap['total_overlaps'],\n",
        "            'unambiguity_score': shap_unambiguity,\n",
        "            'fidelity_disparity': shap_fidelity_disparity,\n",
        "            'stability_disparity': shap_stability_disparity,\n",
        "            'consistency_disparity': shap_consistency_disparity,\n",
        "            'sparsity_disparity': shap_sparsity_disparity,\n",
        "            'completeness': shap_completeness,\n",
        "            'diversity': shap_diversity,\n",
        "            'feature_mutual_information': shap_feature_mi,\n",
        "            'model_parameter_randomization': shap_model_param_rand,\n",
        "            'fidelity': shap_fidelity\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"✗ SHAP evaluation failed: {e}\")\n",
        "        shap_results = None\n",
        "\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # LIME EVALUATION\n",
        "    # ========================================================================\n",
        "\n",
        "\n",
        "    print(\"\\n4. LIME EVALUATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Setup LIME\n",
        "        lime_explainer = LimeTabularExplainer(\n",
        "            scaler.transform(X_train),\n",
        "            feature_names=feature_names,\n",
        "            class_names=[\"Benign\", \"Malignant\"],\n",
        "            discretize_continuous=True,\n",
        "            random_state=42\n",
        "        )\n",
        "        print(\"✓ LIME explainer initialized\")\n",
        "\n",
        "        # Get LIME explanation\n",
        "        def get_lime_explanation(x):\n",
        "            x_scaled = scaler.transform(x.reshape(1, -1))[0]\n",
        "            explanation = lime_explainer.explain_instance(\n",
        "                x_scaled, lstm_predict_proba, num_features=len(feature_names), num_samples=200\n",
        "            )\n",
        "\n",
        "            explanation_dict = dict(explanation.as_list())\n",
        "            attributions = []\n",
        "            for feature_name in feature_names:\n",
        "                weight = 0.0\n",
        "                for lime_feature, lime_weight in explanation_dict.items():\n",
        "                    if feature_name in lime_feature:\n",
        "                        weight = lime_weight\n",
        "                        break\n",
        "                attributions.append(weight)\n",
        "            return np.array(attributions)\n",
        "\n",
        "        lime_explanation = get_lime_explanation(x_sample)\n",
        "        print(f\"✓ LIME explanation shape: {lime_explanation.shape}\")\n",
        "\n",
        "        # Compute LIME metrics\n",
        "        print(\"Computing LIME metrics...\")\n",
        "        lime_infidelity = compute_infidelity(lstm_predict_proba, x_sample, lime_explanation, get_lime_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        lime_complexity = compute_complexity(lime_explanation)\n",
        "        lime_sensitivity = compute_sensitivity(lstm_predict_proba, x_sample, get_lime_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        lime_ris = compute_relative_input_stability(lstm_predict_proba, x_sample, get_lime_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        lime_ros = compute_relative_output_stability(lstm_predict_proba, x_sample, get_lime_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        lime_rrs = compute_relative_representation_stability(lstm_predict_proba, x_sample, get_lime_explanation, get_lstm_representation, n_samples=N_METRIC_SAMPLES)\n",
        "        lime_faith = compute_faithfulness(lstm_predict_proba, x_sample, get_lime_explanation)\n",
        "        lime_fidelity_disparity = compute_fidelity_disparity(lstm_predict_single, group0_samples, group1_samples,get_lime_explanation, k=5, m=20, sigma=0.1)\n",
        "        lime_stability_disparity = compute_stability_disparity(lstm_predict_single, group0_samples, group1_samples,get_lime_explanation, m=5, sigma=0.1)\n",
        "        lime_consistency_disparity = compute_consistency_disparity(lstm_predict_single, group0_samples, group1_samples,get_lime_explanation, m=5)\n",
        "        lime_sparsity_disparity = compute_sparsity_disparity(lstm_predict_single, group0_samples, group1_samples,get_lime_explanation, threshold=0.01)\n",
        "        lime_cover = compute_cover(lstm_predict_single, test_samples, get_lime_explanation)\n",
        "        lime_rule_overlap = compute_rule_overlap(lstm_predict_single, test_samples, get_lime_explanation,similarity_threshold=0.8, top_k_features=5)\n",
        "        lime_unambiguity = compute_unambiguity_score(lstm_predict_single, test_samples, get_lime_explanation)\n",
        "        lime_completeness = compute_completeness(lstm_predict_proba, x_sample, lime_explanation)\n",
        "        lime_diversity = compute_diversity(lstm_predict_single, test_samples, get_lime_explanation, distance_metric='euclidean')\n",
        "        lime_feature_mi = compute_feature_mutual_information(lstm_predict_single, test_samples, get_lime_explanation, threshold=0.1)\n",
        "        lime_model_param_rand = compute_model_parameter_randomization(lstm_predict_proba, x_sample, get_lime_explanation, trained_model = lstm_model,  model_type='tensorflow', randomization_type='full',n_samples=N_METRIC_SAMPLES)\n",
        "        lime_fidelity = compute_fidelity_from_explanations(lstm_predict_proba, x_sample, lime_explanation, get_lime_explanation)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"✓ LIME Results:\")\n",
        "        print(f\"  Infidelity: {lime_infidelity:.4f}\")\n",
        "        print(f\"  Complexity: {lime_complexity:.4f}\")\n",
        "        print(f\"  Sensitivity: {lime_sensitivity:.4f}\")\n",
        "        print(f\"  RIS: {lime_ris:.4f}\")\n",
        "        print(f\"  ROS: {lime_ros:.4f}\")\n",
        "        print(f\"  RRS: {lime_rrs:.4f}\")\n",
        "        print(f\"  Faithfulness: {lime_faith:.4f}\")\n",
        "        print(\"✓ lime Disparity Results:\")\n",
        "        print(f\"  Fidelity Disparity:\")\n",
        "        print(f\"    Group 0 (Benign) Mean: {lime_fidelity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 (Malignant) Mean: {lime_fidelity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {lime_fidelity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"    Disparity Difference: {lime_fidelity_disparity['disparity_difference']:.4f}\")\n",
        "        print(f\"  Stability Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {lime_stability_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {lime_stability_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {lime_stability_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Consistency Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {lime_consistency_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {lime_consistency_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {lime_consistency_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Sparsity Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {lime_sparsity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {lime_sparsity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {lime_sparsity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(\"✓ SHAP Cover and Rule Overlap Results:\")\n",
        "        print(f\"  Cover Rate: {lime_cover['coverage_rate']:.4f}\")\n",
        "        print(f\"  Covered Instances: {lime_cover['covered_instances']}/{lime_cover['total_instances']}\")\n",
        "        print(f\"  Rule Overlap Rate: {lime_rule_overlap['overlap_rate']:.4f}\")\n",
        "        print(f\"  Total Overlaps: {lime_rule_overlap['total_overlaps']}\")\n",
        "        print(f\"  Ambiguous Instances: {lime_rule_overlap['ambiguous_instances']}\")\n",
        "        print(f\"  Unambiguity Score: {lime_unambiguity:.4f}\")\n",
        "        print(f\"✓ LIME Completeness: {lime_completeness:.4f}\")\n",
        "        print(f\"✓ LIME Diversity: {lime_diversity:.4f}\")\n",
        "        print(f\"✓ LIME Feature Mutual Information: {lime_feature_mi:.4f}\")\n",
        "        print(f\"✓ LIME Model Parameter Randomization: {lime_model_param_rand:.4f}\")\n",
        "        print(f\"✓ LIME Fidelity: {lime_fidelity:.4f}\")\n",
        "\n",
        "        lime_results = {\n",
        "            'infidelity': lime_infidelity,\n",
        "            'complexity': lime_complexity,\n",
        "            'sensitivity': lime_sensitivity,\n",
        "            'ris': lime_ris,\n",
        "            'ros': lime_ros,\n",
        "            'rrs': lime_rrs,\n",
        "            'faithfulness': lime_faith,\n",
        "            'cover': lime_cover,\n",
        "            'rule_overlap': lime_rule_overlap,\n",
        "            'total_overlaps': lime_rule_overlap['total_overlaps'],\n",
        "            'unambiguity_score': lime_unambiguity,\n",
        "            'fidelity_disparity': lime_fidelity_disparity,\n",
        "            'stability_disparity': lime_stability_disparity,\n",
        "            'consistency_disparity': lime_consistency_disparity,\n",
        "            'sparsity_disparity': lime_sparsity_disparity,\n",
        "            'completeness': lime_completeness,\n",
        "            'diversity': lime_diversity,\n",
        "            'feature_mutual_information': lime_feature_mi,\n",
        "            'model_parameter_randomization': lime_model_param_rand,\n",
        "            'fidelity': lime_fidelity\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"✗ LIME evaluation failed: {e}\")\n",
        "        lime_results = None\n",
        "\n",
        "\n",
        "\n",
        "    # ========================================================================\n",
        "    # EBM EVALUATION\n",
        "    # ========================================================================\n",
        "    print(\"\\n5. EBM EVALUATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Get EBM explanation\n",
        "        def get_ebm_explanation(x):\n",
        "            x_scaled = scaler.transform(x.reshape(1, -1))\n",
        "            x_df = pd.DataFrame(x_scaled, columns=feature_names)\n",
        "            local_exp = ebm_model.explain_local(x_df)\n",
        "\n",
        "            # Try to extract scores\n",
        "            try:\n",
        "                overall_exp = local_exp.data(0)\n",
        "                if isinstance(overall_exp, dict) and 'scores' in overall_exp:\n",
        "                    scores = np.array(overall_exp['scores'])\n",
        "                    # Handle shape mismatch (interactions)\n",
        "                    if len(scores) > len(feature_names):\n",
        "                        scores = scores[:len(feature_names)]  # Take main effects only\n",
        "                    return scores\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Fallback: feature-by-feature\n",
        "            scores = []\n",
        "            for i in range(len(feature_names)):\n",
        "                try:\n",
        "                    feature_exp = local_exp.data(i)\n",
        "                    if isinstance(feature_exp, dict) and 'scores' in feature_exp:\n",
        "                        score_data = feature_exp['scores']\n",
        "                        if isinstance(score_data, (list, np.ndarray)):\n",
        "                            score = np.mean(score_data) if len(score_data) > 0 else 0.0\n",
        "                        else:\n",
        "                            score = float(score_data)\n",
        "                    else:\n",
        "                        score = 0.0\n",
        "                    scores.append(score)\n",
        "                except:\n",
        "                    scores.append(0.0)\n",
        "\n",
        "            return np.array(scores)\n",
        "\n",
        "        ebm_explanation = get_ebm_explanation(x_sample)\n",
        "        print(f\"✓ EBM explanation shape: {ebm_explanation.shape}\")\n",
        "\n",
        "        # Compute EBM metrics\n",
        "        print(\"Computing EBM metrics...\")\n",
        "        ebm_infidelity = compute_infidelity(ebm_predict_proba, x_sample, ebm_explanation, get_ebm_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_complexity = compute_complexity(ebm_explanation)\n",
        "        ebm_sensitivity = compute_sensitivity(ebm_predict_proba, x_sample, get_ebm_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_ris = compute_relative_input_stability(ebm_predict_proba, x_sample, get_ebm_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_ros = compute_relative_output_stability(ebm_predict_proba, x_sample, get_ebm_explanation, n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_rrs = compute_relative_representation_stability(ebm_predict_proba, x_sample, get_ebm_explanation, get_ebm_representation, n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_faith = compute_faithfulness(ebm_predict_proba, x_sample, get_ebm_explanation)\n",
        "        ebm_fidelity_disparity = compute_fidelity_disparity(ebm_predict_single, group0_samples, group1_samples,get_ebm_explanation, k=5, m=20, sigma=0.1)\n",
        "        ebm_stability_disparity = compute_stability_disparity(ebm_predict_single, group0_samples, group1_samples,get_ebm_explanation, m=5, sigma=0.1)\n",
        "        ebm_consistency_disparity = compute_consistency_disparity(ebm_predict_single, group0_samples, group1_samples,get_ebm_explanation, m=5)\n",
        "        ebm_sparsity_disparity = compute_sparsity_disparity(ebm_predict_single, group0_samples, group1_samples,get_ebm_explanation, threshold=0.01)\n",
        "        ebm_cover = compute_cover(ebm_predict_single, test_samples, get_ebm_explanation)\n",
        "        ebm_rule_overlap = compute_rule_overlap(ebm_predict_single, test_samples, get_ebm_explanation,similarity_threshold=0.8, top_k_features=5)\n",
        "        ebm_unambiguity = compute_unambiguity_score(ebm_predict_single, test_samples, get_ebm_explanation)\n",
        "        ebm_completeness = compute_completeness(ebm_predict_proba, x_sample, ebm_explanation)\n",
        "        ebm_diversity = compute_diversity(ebm_predict_single, test_samples, get_ebm_explanation, distance_metric='euclidean')\n",
        "        ebm_feature_mi = compute_feature_mutual_information(ebm_predict_single, test_samples, get_ebm_explanation, threshold=0.1)\n",
        "        ebm_model_param_rand = compute_model_parameter_randomization(ebm_predict_proba, x_sample, get_ebm_explanation,trained_model = ebm_model,  model_type='sklearn', randomization_type='full',  n_samples=N_METRIC_SAMPLES)\n",
        "        ebm_fidelity = compute_fidelity_from_explanations(ebm_predict_proba, x_sample, ebm_explanation, get_ebm_explanation)\n",
        "\n",
        "        print(\"✓ EBM Results:\")\n",
        "        print(f\"  Infidelity: {ebm_infidelity:.4f}\")\n",
        "        print(f\"  Complexity: {ebm_complexity:.4f}\")\n",
        "        print(f\"  Sensitivity: {ebm_sensitivity:.4f}\")\n",
        "        print(f\"  RIS: {ebm_ris:.4f}\")\n",
        "        print(f\"  ROS: {ebm_ros:.4f}\")\n",
        "        print(f\"  RRS: {ebm_rrs:.4f}\")\n",
        "        print(f\"  Faithfulness: {ebm_faith:.4f}\")\n",
        "        print(\"✓ Ebm Disparity Results:\")\n",
        "        print(f\"  Fidelity Disparity:\")\n",
        "        print(f\"    Group 0 (Benign) Mean: {ebm_fidelity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 (Malignant) Mean: {ebm_fidelity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {ebm_fidelity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"    Disparity Difference: {ebm_fidelity_disparity['disparity_difference']:.4f}\")\n",
        "        print(f\"  Stability Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {ebm_stability_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {ebm_stability_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {ebm_stability_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Consistency Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {ebm_consistency_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {ebm_consistency_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {ebm_consistency_disparity['disparity_ratio']:.4f}\")\n",
        "        print(f\"  Sparsity Disparity:\")\n",
        "        print(f\"    Group 0 Mean: {ebm_sparsity_disparity['group0_mean']:.4f}\")\n",
        "        print(f\"    Group 1 Mean: {ebm_sparsity_disparity['group1_mean']:.4f}\")\n",
        "        print(f\"    Disparity Ratio: {ebm_sparsity_disparity['disparity_ratio']:.4f}\")\n",
        "        print(\"✓ ebm Cover and Rule Overlap Results:\")\n",
        "        print(f\"  Cover Rate: {ebm_cover['coverage_rate']:.4f}\")\n",
        "        print(f\"  Covered Instances: {ebm_cover['covered_instances']}/{ebm_cover['total_instances']}\")\n",
        "        print(f\"  Rule Overlap Rate: {ebm_rule_overlap['overlap_rate']:.4f}\")\n",
        "        print(f\"  Total Overlaps: {ebm_rule_overlap['total_overlaps']}\")\n",
        "        print(f\"  Ambiguous Instances: {ebm_rule_overlap['ambiguous_instances']}\")\n",
        "        print(f\"  Unambiguity Score: {ebm_unambiguity:.4f}\")\n",
        "        print(f\"✓ EBM Completeness: {ebm_completeness:.4f}\")\n",
        "        print(f\"✓ EBM Diversity: {ebm_diversity:.4f}\")\n",
        "        print(f\"✓ EBM Feature Mutual Information: {ebm_feature_mi:.4f}\")\n",
        "        print(f\"✓ EBM Model Parameter Randomization: {ebm_model_param_rand:.4f}\")\n",
        "        print(f\"✓ EBM Fidelity: {ebm_fidelity:.4f}\")\n",
        "\n",
        "        ebm_results = {\n",
        "            'infidelity': ebm_infidelity,\n",
        "            'complexity': ebm_complexity,\n",
        "            'sensitivity': ebm_sensitivity,\n",
        "            'ris': ebm_ris,\n",
        "            'ros': ebm_ros,\n",
        "            'rrs': ebm_rrs,\n",
        "            'faithfulness': ebm_faith,\n",
        "            'cover': ebm_cover,\n",
        "            'rule_overlap': ebm_rule_overlap,\n",
        "            'total_overlaps': ebm_rule_overlap['total_overlaps'],\n",
        "            'unambiguity_score': ebm_unambiguity,\n",
        "            'fidelity_disparity': ebm_fidelity_disparity,\n",
        "            'stability_disparity': ebm_stability_disparity,\n",
        "            'consistency_disparity': ebm_consistency_disparity,\n",
        "            'sparsity_disparity': ebm_sparsity_disparity,\n",
        "            'completeness': ebm_completeness,\n",
        "            'diversity': ebm_diversity,\n",
        "            'feature_mutual_information': ebm_feature_mi,\n",
        "            'model_parameter_randomization': ebm_model_param_rand,\n",
        "            'fidelity': ebm_fidelity\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ EBM evaluation failed: {e}\")\n",
        "        ebm_results = None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796,
          "referenced_widgets": [
            "4cb4b51dcd354e438f3257decf32cc11",
            "4f99ad6c95174561a34abc5728c86331",
            "643e19787bce4a1c953a69b0959975b5",
            "34c1d1fa98954f799a8dbb518cf9515e",
            "aa153375bb9b441e855039f73d086d44",
            "2608407dec9644c481a45f546e323b29",
            "dcfc79642cd24074899c850c9f173361",
            "ae7e6a83c2104718a8d84c4f78b93206",
            "3602a34226734ab0a14520f611946df9",
            "e302026e6eaf4894b2cbb4082a321561",
            "3f2a953acfe34b459f0f7c4851b643c2"
          ]
        },
        "id": "7TheRGMQ5AJW",
        "outputId": "1450525e-21f7-4197-9b9a-812286d26612"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "XAI METRICS EVALUATION\n",
            "================================================================================\n",
            "\n",
            "1. PREPARING DATA\n",
            "--------------------------------------------------\n",
            "✓ Data loaded: 569 samples, 30 features\n",
            "✓ Train set: 455 samples\n",
            "✓ Test set: 114 samples\n",
            "✓ Selected sample 0: True label = 0 (Benign)\n",
            "\n",
            "1.5. PREPARING DEMOGRAPHIC GROUPS\n",
            "--------------------------------------------------\n",
            "✓ Group 0 (Benign): 20 samples\n",
            "✓ Group 1 (Malignant): 20 samples\n",
            "\n",
            "2. LOADING MODELS\n",
            "--------------------------------------------------\n",
            "✓ LSTM model loaded\n",
            "✓ EBM model and components loaded\n",
            "✓ LSTM prediction: 0.3702 (Benign)\n",
            "✓ EBM prediction: 0.0000 (Benign)\n",
            "\n",
            "3. SHAP EVALUATION\n",
            "--------------------------------------------------\n",
            "✓ SHAP explainer initialized\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cb4b51dcd354e438f3257decf32cc11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHAP explanation shape: (30,)\n",
            "Computing SHAP metrics...\n",
            "✓ SHAP Fidelity: 0.9628\n",
            "\n",
            "4. LIME EVALUATION\n",
            "--------------------------------------------------\n",
            "✓ LIME explainer initialized\n",
            "✓ LIME explanation shape: (30,)\n",
            "Computing LIME metrics...\n",
            "✓ LIME Fidelity: 0.8992\n",
            "\n",
            "5. EBM EVALUATION\n",
            "--------------------------------------------------\n",
            "✓ EBM explanation shape: (30,)\n",
            "Computing EBM metrics...\n",
            "✓ EBM Fidelity: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t99GwnGKyFhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2hJ1_ubvSrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cb4b51dcd354e438f3257decf32cc11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f99ad6c95174561a34abc5728c86331",
              "IPY_MODEL_643e19787bce4a1c953a69b0959975b5",
              "IPY_MODEL_34c1d1fa98954f799a8dbb518cf9515e"
            ],
            "layout": "IPY_MODEL_aa153375bb9b441e855039f73d086d44"
          }
        },
        "4f99ad6c95174561a34abc5728c86331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2608407dec9644c481a45f546e323b29",
            "placeholder": "​",
            "style": "IPY_MODEL_dcfc79642cd24074899c850c9f173361",
            "value": "100%"
          }
        },
        "643e19787bce4a1c953a69b0959975b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae7e6a83c2104718a8d84c4f78b93206",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3602a34226734ab0a14520f611946df9",
            "value": 1
          }
        },
        "34c1d1fa98954f799a8dbb518cf9515e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e302026e6eaf4894b2cbb4082a321561",
            "placeholder": "​",
            "style": "IPY_MODEL_3f2a953acfe34b459f0f7c4851b643c2",
            "value": " 1/1 [00:00&lt;00:00,  1.70it/s]"
          }
        },
        "aa153375bb9b441e855039f73d086d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2608407dec9644c481a45f546e323b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfc79642cd24074899c850c9f173361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae7e6a83c2104718a8d84c4f78b93206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3602a34226734ab0a14520f611946df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e302026e6eaf4894b2cbb4082a321561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2a953acfe34b459f0f7c4851b643c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}